#!/usr/bin/python2
# -*- coding: utf-8 -*-
# -*- mode: Python; fill-column: 75; comment-column: 50; -*-
#
# (c) iomonad - <clement@trosa.io>
#
# Description: BNF.fr ressource feching script.
# Dependencies:
#  - Python 2.7
#  - mozilla/geckodriver: (LATEST)
#

import os
import sys
import time
import uuid
import requests
import logging
import Queue
import threading

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Referential url with document variable
scheme = "https://bibliotheques-specialisees.paris.fr/ark:/73873/pf0000855431/{:04d}/v0001.simple"

# Convenient logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
fh = logging.FileHandler('bnfetcher.log')
ch = logging.StreamHandler()
fh.setLevel(logging.DEBUG)
ch.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
fh.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)

class Crawler:
    """Generic boilerplate for naive
    crawling directives"""

    def __init__(self, driver, timeout=5, directory="."):
        self.driver = driver
        self.timeout = timeout
        self.directory = directory
        self.pool = Queue.Queue()

    def target(self, page):
        "Format target URL"
        return scheme.format(page)

    def generate(self, res):
        "Generate UUID package from BNF's server"
        assert res >= 1
        self.driver.get(self.target(res))
        title = self.driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[2]/div[1]/a').text.encode("ascii","ignore")
        self.driver.find_element(By.XPATH, '//*[@id="hires"]').click()
        self.driver.find_element(By.XPATH,
                                 '//*[@id="checkConditions"]').click()
        self.driver.find_element(
            By.XPATH, '/html/body/div[5]/div[3]/div/button').click()
        try:
            WebDriverWait(self.driver, self.timeout).until(EC.alert_is_present(),
                                        'Timed out waiting for PA creation ' +
                                            'confirmation popup to appear.')
            alert = self.driver.switch_to.alert
            alert.dimiss()
        except TimeoutException:
            logger.info("Processing resource: {}".format(title))
        return self.driver.find_element(
            By.XPATH, '/html/body/div[5]/div[3]/div/a').get_attribute('href')

    def download(self, archive):
        "Pooling download lambda"
        r = requests.get(archive, stream=True)
        t = "{}.zip".format(str(uuid.uuid4()))
        logger.info("Downloading archive {} to target {}.".format(archive, t))
        if r.status_code == 200:
            with open(t, 'wb') as f:
                for chunk in r:
                    f.write(chunk)
            logger.info("Successfuly downloaded archive {} to target {}.".format(archive, t))
        else:
          logger.error("Error while fetching ark resources for: {}.".format(archive))

    def compute(self, rate):
        "Naive pooled loop processing"
        for doc in range(1, rate):
            url = self.generate(doc)
            logger.info("Pushing ARK resource to pool: {}".format(url))
            self.download(url)
    def __del__(self):
        self.driver.close()
        while not self.pool.empty:
            pass

if __name__ == "__main__":
    logger.info("Starting BNF fetcher")
    try:
        crawler = Crawler(webdriver.Firefox())
        crawler.compute(3)
    except KeyboardInterrupt:
        sys.exit(0)
