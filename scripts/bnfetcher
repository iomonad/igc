#!/usr/bin/python2
# -*- mode: Python; fill-column: 75; comment-column: 50; -*-
#
# (c) iomonad - <clement@trosa.io>
#
# Description: BNF.fr ressource feching script.
# Dependencies:
#  - Python 2.7
#  - mozilla/geckodriver: (LATEST)
#

import uuid
import request
import os, sys, time
from selenium import webdriver
from selenium.webdriver.common.by import By
from multiprocessing.pool import ThreadPool
from selenium.webdriver.support.ui import WebDriverWait

# Referential url with document variable
scheme = "https://bibliotheques-specialisees.paris.fr/ark:/73873/pf0000855431/{:04d}/v0001.simple"

class Crawler:
    """Generic boilerplate for naive
    crawling directives"""
    def __init__(self, driver, timeout = 5, directory = "."):
        self.driver = driver
        self.timeout = timeout
        self.directory = directory
    def target(self, page):
        return scheme.format(page)
    def generate(self, res):
        """
        Generate UUID package from BNF's server
        and return url for pooling download process
        """
        assert res >= 1
        self.driver.get(self.target(res))
        self.driver.find_element(By.XPATH, '//*[@id="hires"]').click()
        self.driver.find_element(By.XPATH, '//*[@id="checkConditions"]').click()
        self.driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div/button').click()
        time.sleep(self.timeout)
        return self.driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div/a').get_attribute('href')
    def download(self, archive):
        """Pooling download lambda"""
        r = requests.get(archive, stream = True)
        t = "{}.zip".format(str(uuid.uuid4()))
        if r.status_code == 200:
            with open(path, 'wb') as f:
                for chunk in r:
                    f.write(chunk)
    def compute(self, rate):
        # TODO: Add download pool
        """
        Process naive paginated fetching
        """
        for doc in range(1, rate):
            print self.generate(doc)
    def __del__(self):
        self.driver.close()

if __name__ == "__main__":
    crawler = Crawler(webdriver.Firefox())
    crawler.compute(10)
