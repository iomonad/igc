#!/usr/bin/python2
# -*- coding: utf-8 -*-
# -*- mode: Python; fill-column: 75; comment-column: 50; -*-
#
# (c) iomonad - <clement@trosa.io>
#
# Description: BNF.fr ressource feching script.
# Dependencies:
#  - Python 2.7
#  - mozilla/geckodriver: (LATEST)
#

import os
import sys
import time
import uuid
import requests
import logging
import threadpool

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Referential url with document variable
scheme = "https://bibliotheques-specialisees.paris.fr/ark:/73873/pf0000855431/{:04d}/v0001.simple"

artefacts = 0x155

# Convenient logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
fh = logging.FileHandler('bnfetcher.log')
ch = logging.StreamHandler()
fh.setLevel(logging.DEBUG)
ch.setLevel(logging.INFO)
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
fh.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)


class Crawler:
    """Generic boilerplate for naive
    crawling directives"""

    def __init__(self, driver, timeout=10, directory="archives"):
        self.driver = driver
        self.timeout = timeout
        self.directory = directory
        self.pool = threadpool.ThreadPool(4)
        if not (os.path.isdir(self.directory)):
            os.makedirs(self.directory)

    def target(self, page):
        "Format target URL"
        return scheme.format(page)

    def generate(self, res):
        "Generate UUID package from BNF's server"
        assert res >= 1
        self.driver.get(self.target(res))
        try:
            title = self.driver.find_element(
                By.XPATH,
                '/html/body/div[1]/div[1]/div[1]/div[2]/div[1]/a').text.encode(
                    "ascii", "ignore")
            self.driver.find_element(By.XPATH, '//*[@id="hires"]').click()
            self.driver.find_element(By.XPATH,
                                     '//*[@id="checkConditions"]').click()
            self.driver.find_element(
                By.XPATH, '/html/body/div[5]/div[3]/div/button').click()
        except NoSuchElementException, e:
            logger.error("Invalid XPATH directive: {}".format(e.message))
        try:
            WebDriverWait(self.driver, self.timeout).until(
                EC.alert_is_present(), 'Timed out waiting for PA creation ' +
                'confirmation popup to appear.')
            alert = self.driver.switch_to.alert
            alert.dimiss()
        except TimeoutException:
            logger.info("Processing resource: {}".format(title))
        return {
            'aref':
            self.driver.find_element(
                By.XPATH,
                '/html/body/div[5]/div[3]/div/a').get_attribute('href'),
            'title':
            title
        }

    def download(self, archive):
        "Pooling download lambda"
        r = requests.get(archive['aref'], stream=True)
        t = "{}/{}.zip".format(self.directory, archive['title'].replace(
            " ", "_"))
        if (os.path.isfile(t)):
            return logger.info(
                "File {} already exist, skipping download.".format(t))
        logger.info("Downloading archive {} to target {}.".format(
            archive['aref'], t))
        if r.status_code == 200:
            with open(t, 'wb') as f:
                for chunk in r:
                    f.write(chunk)
            logger.info(
                "Successfuly downloaded archive {} to target {}.".format(
                    archive['aref'], t))
        else:
            logger.error("Error while fetching ark resources for: {}.".format(
                archive['aref']))

    def compute(self, rate):
        "Naive pooled loop processing"
        for doc in range(1, rate):
            try:
                url = self.generate(doc)
            except Exception, e:
                logger.fatal(
                    "ERROR while processing [{}] - Recording failure for manual processing."
                    .format(doc))
                continue
            logger.info("Pushing ARK resource to pool: {}".format(url))
            [
                self.pool.putRequest(x)
                for x in threadpool.makeRequests(self.download, [url])
            ]
            self.pool.wait()

    def __del__(self):
        self.driver.close()


if __name__ == "__main__":
    logger.info("Starting BNF fetcher")
    try:
        crawler = Crawler(webdriver.Firefox())
        crawler.compute(artefacts)
    except KeyboardInterrupt:
        sys.exit(0)
